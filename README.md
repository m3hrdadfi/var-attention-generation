
# Variational Attention for Sequence to Sequence Models Using Dirichlet Distribution

![](https://img.shields.io/badge/python-3.5-brightgreen.svg) ![](https://img.shields.io/badge/tensorflow-1.10.0-orange.svg)

Implementation of 'Variational Attention for Sequence to Sequence Models Using Dirichlet Distribution' in TensorFlow 
based on [Variational Attention Research](https://arxiv.org/pdf/1712.08207.pdf).

## Overview
In practice, the title generation from long texts is exceptionally challenging. On the other hand, the diversity of 
context in the data used is high, and this can affect the learning path. Also, if the architecture of the network 
is not well designed, the attention mechanism may impede the process of training, and the system cannot learn valuable 
information, instead of learning to go into ineffective learning which it is called "Bypassing.". We propose a 
variational encoder-decoder model along with the attention mechanism where the VED and the attention vector modeled 
as Dirichlet distributed.

Dirichlet distributions often use as priors over comparable data. Latent Dirichlet Allocation, for example, suggests 
that there are n topics and assigns a $$P(\theta_{i})$$ of a data point being on a particular issue. Many models like 
soft-clustering or matrix factorization seek to find underlying categories and assigns proportions to each group. 
The Dirichlet Prior to our latent space is used similarly to learn non-linear mappings to a mixture of classes. 
The difficulty lies in that we cannot reparameterize the Dirichlet for sampling the same way that we can with a 
Normal distribution. Therefore we must find an alternative way to reparametrize the sample. An alternative for the 
Dirichlet distribution would be a Logistic-Normal generated by applying the logistic function $$L(x) = \frac{e^{x}}{\sum e^{x}}$$ to 
samples from a Normal. One method of approximating a Dirichlet is to find the Logistic-Normal with $$\mu, \sigma$$ as a 
function of $$\alpha$$ by minimizing the $$D_{KL}$$.
$$$
D_{KL}(P_{\alpha} \parallel q_{\beta}) = \log\Gamma(\alpha_{0}) - \sum_K \log\Gamma(\alpha_{k}) + \sum_K \alpha_{k} \psi(\alpha_{k}) - \frac{1}{K}\left[\psi(\alpha_{k}) - \psi(\alpha_{0}) \right]
$$$

## Datasets
Our dataset consists of the 141794 News content between the years of 2016 and July 2017 from famous publications include 
the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, 
National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The data was scraped using 
BeautifulSoup and segmented into nine sections (Id, Title, Publication, Author, Date, Year, Month, URL, Content) 
provided by [Andrew Thompson and shared at Kaggle](https://www.kaggle.com/snapcrack/all-the-news/home). 
The figure shows the abundance of news concerning the publisher.

Our dataset consists of the 141794 News content between the years of 2016 and July 2017 from famous publications include the New York Times, Breitbart, CNN, Business Insider, the Atlantic, Fox News, Talking Points Memo, Buzzfeed News, National Review, New York Post, the Guardian, NPR, Reuters, Vox, and the Washington Post. The data was scraped using BeautifulSoup and segmented into nine sections (Id, Title, Publication, Author, Date, Year, Month, URL, Content) provided by [Andrew Thompson and shared at Kaggle](https://www.kaggle.com/snapcrack/all-the-news/home). Figure A shows the abundance of news concerning the publisher.
So the initial analysis of the data has revealed that the data has duplicate records both in the rows and in the publication's column. Also, texts in this corpus were not Unicode. Because the content columns have a high- level of information along with needed for higher computations, we decided to extract the excerpts from them and use them as inputs instead of examining the entire news content. These inputs were also shorter in some records than the output length. After solving all the above issues with the help of preprocessing techniques, the final abundance of news concerning the publisher shows in Figure B.
![alt text][dataset]

## Requirements
- tensorflow-gpu==1.10.0
- Keras==2.2.3
- numpy==1.15.2
- pandas==0.23.4
- gensim==3.4.0
- nltk==3.3
- tqdm==4.26.0

## Word embeddings
- numberbatch v17.06(en)


[dataset]: info/new-abundance-o.png "Dataset distibution"